{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c4245-1189-4ed9-9f07-a204f0cdc180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients using environment variables\n",
    "openai_client = OpenAI()  # Will automatically use OPENAI_API_KEY env var\n",
    "anthropic_client = anthropic.Anthropic()  # Will automatically use ANTHROPIC_API_KEY env var\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# DeepSeek uses OpenAI-compatible API\n",
    "deepseek_client = OpenAI(\n",
    "    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    base_url=\"https://api.deepseek.com/v1\"\n",
    ")\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv(\"personalization_data.csv\")\n",
    "\n",
    "# Define the catalog (you'll need to replace this with your actual catalog)\n",
    "catalog = [\"Electronics\", \"Fashion\", \"Home & Garden\", \"Sports\", \"Books\", \n",
    "           \"Beauty\", \"Toys\", \"Food & Grocery\", \"Health\", \"Automotive\"]\n",
    "\n",
    "def create_prompt(row):\n",
    "    \"\"\"Create prompt from user data\"\"\"\n",
    "    user_data = {\n",
    "        \"linkedin_headline\": row[\"linkedin_headline\"],\n",
    "        \"liked_tweets\": [row[\"liked_tweets.0\"], row[\"liked_tweets.1\"], row[\"liked_tweets.2\"]]\n",
    "    }\n",
    "    \n",
    "    prompt = (\n",
    "        f'json {json.dumps(user_data)}\\n'\n",
    "        f'\"From the catalog below, pick the top two categories this user will most likely engage with '\n",
    "        f'and justify each in â‰¤25 words. Return JSON: {{ \"picks\": [\"...\", \"...\"], \"why\": [\"...\", \"...\"] }}\"\\n'\n",
    "        f'Catalog: {\", \".join(catalog)}'\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def call_chatgpt(prompt):\n",
    "    \"\"\"Call ChatGPT API\"\"\"\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"ChatGPT error: {e}\")\n",
    "        return None\n",
    "\n",
    "def call_claude(prompt):\n",
    "    \"\"\"Call Claude API\"\"\"\n",
    "    try:\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        # Extract JSON from response\n",
    "        content = response.content[0].text\n",
    "        json_start = content.find('{')\n",
    "        json_end = content.rfind('}') + 1\n",
    "        if json_start != -1 and json_end != 0:\n",
    "            return json.loads(content[json_start:json_end])\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Claude error: {e}\")\n",
    "        return None\n",
    "\n",
    "def call_gemini(prompt):\n",
    "    \"\"\"Call Gemini API\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        response = model.generate_content(prompt)\n",
    "        # Extract JSON from response\n",
    "        content = response.text\n",
    "        json_start = content.find('{')\n",
    "        json_end = content.rfind('}') + 1\n",
    "        if json_start != -1 and json_end != 0:\n",
    "            return json.loads(content[json_start:json_end])\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini error: {e}\")\n",
    "        return None\n",
    "\n",
    "def call_deepseek(prompt):\n",
    "    \"\"\"Call DeepSeek API\"\"\"\n",
    "    try:\n",
    "        response = deepseek_client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        json_start = content.find('{')\n",
    "        json_end = content.rfind('}') + 1\n",
    "        if json_start != -1 and json_end != 0:\n",
    "            return json.loads(content[json_start:json_end])\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"DeepSeek error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run experiment\n",
    "results = []\n",
    "models = {\n",
    "    \"ChatGPT\": call_chatgpt,\n",
    "    \"Claude\": call_claude,\n",
    "    \"Gemini\": call_gemini,\n",
    "    \"DeepSeek\": call_deepseek\n",
    "}\n",
    "\n",
    "# Process first 25 users\n",
    "for idx, row in data_df.head(25).iterrows():\n",
    "    prompt = create_prompt(row)\n",
    "    user_id = row[\"user_id\"]\n",
    "    \n",
    "    for model_name, model_func in models.items():\n",
    "        print(f\"Processing user {user_id} with {model_name}...\")\n",
    "        response = model_func(prompt)\n",
    "        \n",
    "        if response:\n",
    "            result = {\n",
    "                \"user_id\": user_id,\n",
    "                \"model\": model_name,\n",
    "                \"response\": response,\n",
    "                \"ground_truth_1\": row[\"cat_gt_1\"],\n",
    "                \"ground_truth_2\": row[\"cat_gt_2\"]\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "# Save results\n",
    "output_dir = Path(\"runs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for model_name in models.keys():\n",
    "    model_dir = output_dir / model_name\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save JSON responses\n",
    "    model_results = [r for r in results if r[\"model\"] == model_name]\n",
    "    with open(model_dir / \"p4.json\", \"w\") as f:\n",
    "        json.dump(model_results, f, indent=2)\n",
    "\n",
    "# Compute hit rates\n",
    "def compute_metrics(results):\n",
    "    \"\"\"Compute top-2 hit rate for each model\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        model_results = [r for r in results if r[\"model\"] == model_name]\n",
    "        hits = 0\n",
    "        total = 0\n",
    "        \n",
    "        for result in model_results:\n",
    "            if result[\"response\"] and \"picks\" in result[\"response\"]:\n",
    "                picks = result[\"response\"][\"picks\"]\n",
    "                gt_1 = result[\"ground_truth_1\"]\n",
    "                gt_2 = result[\"ground_truth_2\"]\n",
    "                \n",
    "                if gt_1 in picks or gt_2 in picks:\n",
    "                    hits += 1\n",
    "                total += 1\n",
    "        \n",
    "        hit_rate = hits / total if total > 0 else 0\n",
    "        metrics[model_name] = {\n",
    "            \"hit_rate\": hit_rate,\n",
    "            \"total\": total,\n",
    "            \"hits\": hits\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "metrics = compute_metrics(results)\n",
    "print(\"\\nMetrics Summary:\")\n",
    "for model, metric in metrics.items():\n",
    "    print(f\"{model}: Hit Rate = {metric['hit_rate']:.2%} ({metric['hits']}/{metric['total']})\")\n",
    "\n",
    "# Create evaluation spreadsheet template\n",
    "eval_data = []\n",
    "for result in results:\n",
    "    if result[\"response\"] and \"picks\" in result[\"response\"] and \"why\" in result[\"response\"]:\n",
    "        eval_data.append({\n",
    "            \"user_id\": result[\"user_id\"],\n",
    "            \"model\": result[\"model\"],\n",
    "            \"pick_1\": result[\"response\"][\"picks\"][0] if len(result[\"response\"][\"picks\"]) > 0 else \"\",\n",
    "            \"pick_2\": result[\"response\"][\"picks\"][1] if len(result[\"response\"][\"picks\"]) > 1 else \"\",\n",
    "            \"rationale_1\": result[\"response\"][\"why\"][0] if len(result[\"response\"][\"why\"]) > 0 else \"\",\n",
    "            \"rationale_2\": result[\"response\"][\"why\"][1] if len(result[\"response\"][\"why\"]) > 1 else \"\",\n",
    "            \"ground_truth_1\": result[\"ground_truth_1\"],\n",
    "            \"ground_truth_2\": result[\"ground_truth_2\"],\n",
    "            \"plausibility_1\": \"\",  # To be manually filled\n",
    "            \"plausibility_2\": \"\"   # To be manually filled\n",
    "        })\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "eval_df.to_csv(\"experiment_results_for_evaluation.csv\", index=False)\n",
    "print(\"\\nEvaluation template saved to 'experiment_results_for_evaluation.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

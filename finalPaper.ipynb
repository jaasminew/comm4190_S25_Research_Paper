{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2634dd-374f-4d5c-a431-bdacbe00b265",
   "metadata": {},
   "source": [
    "# AI in Marketing\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Marketing teams across industries are already weaving generative AI into day‑to‑day workflows—from AI copywriting assistants that draft product descriptions to predictive tools that micro‑segment audiences in real time. As McKinsey observes, “generative AI is poised to be a catalyst for a new age of marketing capabilities” (Harkness et al., 2023, para. 1). Beyond inspiration, the economic stakes are substantial: the same analysis estimates that marketing productivity could rise by 5–15 percent, worth roughly US \\$463 billion annually, while generative AI overall may unlock up to US \\$4.4 trillion in global value. Brands that deploy these tools early report double‑digit lifts in click‑through, conversion, and content velocity, underscoring how quickly AI‑enabled competitors can out‑pace traditional approaches.\n",
    "\n",
    "Generative AI’s impact on marketing coalesces around four capability pillars—targeting, personalization, content generation, and ad optimization. **Targeting** systems draws on user‑modeling methods that “predict a user’s response to specific ad content” from browsing, purchase, and social footprints (Gao et al., 2019, p. 4). **Personalization** builds on those insights via recommendation engines that score each impression against a consumer’s likely preferences and emotional state, pushing ads that resonate and boost engagement. **Content generation** uses LLMs and other generative models to spin up copy, visuals, and scripts at scale while preserving brand voice and experimenting with tonal variants. Finally, **ad optimization** completes the loop, dynamically selecting placements, bids, and creative variants to maximise return on investment. Taken together, targeting and ad optimization revolve around data‑driven matching, whereas personalization and content generation rely on the creative flexibility that LLMs uniquely provide.\n",
    "\n",
    "This study therefore concentrates on the two pillars where Large Language Models have the most direct, text‑centric impact—personalization and content generation—because they combine clear business upside (higher relevance, stronger persuasion, faster creative iteration) with the unique linguistic strengths of LLMs in style transfer, tone control, and idea expansion. Accordingly, we pose a guiding research question: **How effectively do frontier and open‑source LLMs tailor marketing messages to fine‑grained consumer profiles, and which models excel at which specific marketing tasks?** By answering this question, the paper offers marketers an evidence‑based roadmap for selecting the right model‑task pairings that translate generative‑AI hype into tangible campaign lift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7a8ea-1670-4c97-a493-3fba348f7948",
   "metadata": {},
   "source": [
    "## Literature Review\n",
    "### Methodological Landscape\n",
    "Four recent studies illustrate how scholars are beginning to interrogate LLM‑driven personalisation and content generation through distinct research designs and metrics. Matz et al. conducted four preregistered experiments (N ≈ 4,100) in which ChatGPT‑3.5 produced ads that were either trait‑matched or generic; message effectiveness was captured with 7‑point persuasiveness scales and incentive‑compatible willingness‑to‑pay bids (Matz et al., 2024). Brand, Israeli, and Ngwe treated GPT‑3.5‑Turbo as a synthetic respondent in a conjoint survey, comparing its choice shares and WTP coefficients with parallel human data and showing that a lightweight fine‑tune further narrowed prediction error (Brand et al., 2023). Aguilar and Garcia built an adaptive Facebook ad‑creation system that pairs a genetic algorithm for copy mutation with an SVM image selector; performance is logged every seven days via click‑through rate, ad‑frequency rank, and cost‑per‑click (Aguilar & Garcia, 2018). Finally, Gao et al. used VOSviewer bibliometrics to map 241 Scopus papers across targeting, personalisation, content creation, and optimisation, providing a quantitative baseline of where empirical evidence is—and is not—accumulating (Gao et al., 2023).\n",
    "\n",
    "### What the Evidence Says About Effectiveness\n",
    "Collectively, the evidence indicates that generative AI already delivers measurable uplifts when used for personalised content creation. In Matz et al. (2024), 61 % of 33 trait‑matched ads out‑performed their generic counterparts; for example, an iPhone message tailored to extraverts raised perceived effectiveness by 0.25 SD and increased willingness‑to‑pay by the cash equivalent of $33 (Matz et al., 2024). Brand, Israeli, and Ngwe (2023) showed that GPT synthetic respondents reproduced aggregate market‑share estimates within 1–11 percentage points of human baselines, and that a small fine‑tune on legacy surveys further tightened WTP alignment for new product features (Brand et al., 2023). On the executional side, Aguilar and García’s adaptive Facebook system propelled novel creatives from an average rank of 6 to the top‑two slots after six optimisation cycles while lifting click‑through rate and holding cost‑per‑click steady (Aguilar & García, 2018). Industry cases summarised in Gao et al. (2023) reinforce these quantitative signals: Lexus’s AI‑scripted television spot and McCann’s “AI Creative Director” campaign both out‑performed human‑written baselines on like‑through‑rate, and Dynamic Creative Optimisation studies report double‑digit conversion gains when copy, imagery, and offers are assembled in real time from generative components (Gao et al., 2023). Taken together, these findings suggest that AI‑driven personalisation and content generation can boost persuasive impact, predictive accuracy, and media efficiency across multiple stages of the marketing funnel.\n",
    "\n",
    "### Limitations and Research Gaps\n",
    "Despite this encouraging evidence, current work remains bounded in important ways.  Most empirical tests rely on short text ads and self‑report scales rather than the richer creative assets (social posts, video scripts, multiframe stories) encountered in practice; even the sophisticated Facebook study optimises only headline‑image‑size triads and excludes brand voice or compliance constraints.  Synthetic‑respondent papers acknowledge that LLMs still struggle to capture segment‑level heterogeneity and extreme‑tail preferences, while bibliometric mapping highlights a scarcity of task‑level benchmarks inside the “content creation” cluster.  Moreover, cross‑model comparisons are rare—few studies ask which LLM excels at which creative task.  Responding to these gaps, the present paper concentrates exclusively on personalisation and content generation, and does so at the artefact level: copywriting for ads and e‑mails, full social‑media packages, and short‑form video scripts.  Our core research question therefore becomes: How do leading LLMs differ in their ability to generate high‑quality, brand‑consistent marketing content across specific creative tasks, and which models are best suited to which jobs?  By answering this question with systematic, task‑based evaluations, we aim to extend the literature beyond generic persuasion tests toward actionable guidance for practitioners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba176d62-17aa-4b9e-a700-be66b2c81f56",
   "metadata": {},
   "source": [
    "## Testing with LLMs\n",
    "### Methodology\n",
    "To assess the practical marketing capability of today’s frontier LLMs we designed two tightly‑controlled, task‑based experiments—one centred on personalisation and one on personalised content generation.  Both experiments were run with the same three models (OpenAI GPT‑4o, Anthropic Claude 3.5 Sonnet, and Google Gemini 1.5 Pro) under identical temperature (= 0.7) to ensure result comparability.\n",
    "\n",
    "Because real consumer data were unavailable, I constructed a synthetic yet internally consistent evaluation corpus with GPT‑4o. It comprises twenty‑five internet user profiles stored in personalization_data.csv. Each profile includes a LinkedIn headline, a set of liked tweets, three recent web events, dwell‑time information, SKU views, two ground‑truth product‑category preferences and a unique user ID. The corpus also contains ten product briefs saved in content_data.csv; each brief specifies the product, campaign theme, core value proposition, target audience, specification bullets, SEO keywords and a campaign ID. I manually reviewed every profile and brief to ensure topical variety and realism.\n",
    "\n",
    "For the personalisation task my objective was to predict the two product categories with which a user is most likely to engage. I supplied each model with the complete JSON representation of a profile and asked it to list the two categories, ranked first and second, without further explanation. I then calculated Hit@1—the percentage of cases in which the top prediction matched either of the ground‑truth categories—and Hit@2, which records whether at least one of the two predictions matched. These metrics are standard in recommender‑system testing and are easy for marketers to interpret. \n",
    "\n",
    "The personalised content‑generation task aimed to produce short, persuasive blurbs tailored to individual recipients. I paired the first five user profiles with all ten product briefs, creating fifty prompts for each model and 150 generations overall. Each prompt included the user profile, the product brief and an instruction to write a 50–80‑word blurb that referenced the user’s interests or professional context and highlighted the campaign theme and value proposition, avoiding bullet points. After generation I rated every output on two axes. Personalisation Adequacy measured how convincingly the text wove specific user attributes into the pitch; Product Relevance assessed how accurately and completely the blurb conveyed the product’s features and value proposition. Both dimensions used a three‑level Likert scale with scores of one, three or five. I also recorded the word count and then computed the mean personalisation score, mean product relevance score and average length for each model.\n",
    "\n",
    "By grounding both tasks in a controlled, shareable corpus and applying transparent metrics, this methodology provides a reproducible baseline for benchmarking how well current LLMs handle marketing‑relevant personalisation and content‑crafting tasks.\n",
    "\n",
    "### Testing Results\n",
    "In the personalisation task I analysed seventy‑five predictions—twenty‑five from each of the three models. When correctness is defined as nailing both of the customer’s preferred categories (order‑independent set accuracy), the average hit rate of the models is 73.33%, with Claude scored the highest 76.00% (19/25 hits). ChatGPT and Gemini each scored 72.00% (18/25 hits). Qualitatively the rationales were coherent and methodical: the models drew plausible links between LinkedIn headlines, Twitter likes and recent web behaviour to justify their choices. This suggests the underlying preference‑inference capability is already strong. Nonetheless, I expect accuracy to erode once the category taxonomy becomes more granular and once real‑world profiles introduce the messiness of sparse or contradictory signals. Under those tougher conditions the models will need richer first‑party data—especially transaction histories and longitudinal click‑stream records—to stay reliable.\n",
    "\n",
    "For the personalised content‑generation task I evaluated one hundred and fifty blurbs—fifty per model—against a two‑axis rubric. GPT‑4o led comfortably, averaging 0.23 on the 0‑to‑1 personalisation scale and 0.20 on product relevance. Gemini followed with 0.19 personalisation and 0.06 relevance, while Claude lagged at 0.15 and 0.04 respectively. GPT‑4o’s copy was also the longest, about seventy‑one words on average, versus sixty for Claude and fifty‑five for Gemini. The extra length gave GPT‑4o more room to integrate user cues and product benefits, but even its blurbs rarely ventured beyond dropping the customer’s job title or sector into otherwise boilerplate messaging. All three systems treated the assignment as a slot‑filling exercise rather than genuine narrative tailoring, an unsurprising limitation given that I fed them only sparse, job‑centric profile data. As a result the copy, while fluent and technically correct, feels formulaic and lacks the emotional nuance that seasoned copywriters aim for.\n",
    "\n",
    "Taken together, these two experiments show that frontier LLMs already offer a solid baseline for high‑level personalisation and for producing competent marketing copy with minimal prompting. They still fall short, however, of the fine‑grained preference modelling that powers best‑in‑class recommender engines and of the deep persona‑specific storytelling that moves conversion metrics in competitive markets. With broader first‑party data, tighter domain fine‑tuning and retrieval‑augmented grounding I expect performance to improve markedly, but for now I regard the models chiefly as drafting assistants and insight generators rather than fully autonomous personalisation engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c1d75-7589-4d2f-9ac0-baa1380ad8fa",
   "metadata": {},
   "source": [
    "## Experiment Limitations\n",
    "\n",
    "Although the two experiments provide a useful first glimpse into how frontier language models handle marketing tasks, they remain an intentionally simplified abstraction of real‑world practice and therefore carry several important caveats. First, every input profile and product brief was synthesised with GPT‑4o rather than sourced from actual consumers or catalogues. Synthetic data are convenient for privacy and reproducibility, yet they inevitably inherit the distributional quirks, cultural framing and knowledge gaps already present in the generating model. Moreover, each profile covered only a single behavioural channel—essentially a slice of LinkedIn, X and a handful of browsing events—while leading advertising platforms routinely fuse dozens of first‑party and partner signals (purchase logs, mobile‑app telemetry, household graphs, location trails, CRM attributes and so forth). In richer multi‑channel settings, models would have to reconcile contradictory cues and assign differential weights, tasks that go well beyond the comparatively clean inputs used here.\n",
    "\n",
    "Second, the personalisation benchmark focused on predicting broad product categories rather than concrete SKU‑level offers, pricing bundles or creative variants. Hitting the right category is an easier target that inflates apparent precision; translating that broad insight into a timely, specific and margin‑optimised recommendation is far harder and was not measured. Likewise, the content‑generation task asked for short blurbs under uniform length constraints, ignoring the diversity of real campaign deliverables such as multi‑frame carousels, dynamic‑keyword‑insert ads, rich‑media email modules and voice‑assistant scripts. A fuller evaluation would test whether models can handle those varied formats, adapt tone across channels and respect strict brand‑safety guidelines.\n",
    "\n",
    "Third, I deliberately kept the prompts austere—only minimal instructions plus formatting constraints—to eliminate hand‑tuned advantages for any provider. While this choice clarifies a baseline, it also understates what is possible with modern prompt‑engineering techniques such as persona seeding, reference exemplar chains, retrieval‑augmented context or tool‑calling hooks that inject live product feeds. Future studies should compare out‑of‑the‑box performance with scenarios in which each model is given its best‑practice prompt stack and any proprietary grounding tools the vendor offers.\n",
    "\n",
    "Finally, the evaluation rubric, although systematic, relied on a single rater; inter‑rater reliability and consumer‑side A/B testing would yield stronger evidence. The sample size—twenty‑five profiles and ten products—limits statistical power, and the discrete Likert scoring compresses nuanced differences. None of the tests examined model latency, cost per thousand tokens or privacy compliance, all critical factors in production marketing stacks.\n",
    "\n",
    "For all these reasons, the present results should be interpreted as directional rather than definitive benchmarks, and future work should widen the data sources, task granularity and evaluation lenses to approximate the complexity of operational marketing environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5b533-b627-41ff-acf2-4c31a7bfa753",
   "metadata": {},
   "source": [
    "## Implications\n",
    "\n",
    "The findings, provisional though they are, indicate that large‑language‑model systems already possess a practical capacity to infer user intent from multi‑signal behavioural traces and to generate copy that feels at least superficially tailored to individual recipients. Even the simplified tests conducted here show that an off‑the‑shelf model can pick the right product category for seven out of ten synthetic consumers and can weave occupational or industry cues into marketing prose with limited prompting. That baseline competence has important ramifications for the economics and organisation of digital advertising.\n",
    "\n",
    "First, it points toward a step‑change in media‑buying efficiency. If preference inference continues to improve, campaign managers will be able to reduce the volume of impressions required to achieve the same (or higher) conversion goals because ads will be delivered primarily to people whose latent interests align with the offer. Lower impression counts translate directly into smaller auction budgets and a flatter cost curve for customer acquisition. Over time, the marginal savings may allow smaller brands—traditionally priced out of large‑scale programmatic campaigns—to compete with incumbents on near‑equal footing, potentially fragmenting previously consolidated media markets.\n",
    "\n",
    "Second, the creative pipeline itself will accelerate. An LLM that can draft dozens of persona‑specific blurbs or headline variants in seconds reduces the need for junior copywriters to produce first‑round concepts and frees senior talent to focus on brand narrative, conceptual art direction and strategic oversight. Versioning for A/B or multivariate testing becomes an automated, data‑driven loop rather than a manual rewrite process. As models improve at multimodal generation, the same acceleration will extend to image and video assets, shrinking production timelines from weeks to days and allowing real‑time optimisation of creative during live campaigns.\n",
    "\n",
    "These gains, however, depend on access to robust, permissioned first‑party data. Privacy regulation is tightening globally, third‑party cookies are disappearing and mobile platforms are restricting cross‑app identifiers. In that environment the advertiser that owns a deep reservoir of first‑party event streams—transaction histories, loyalty‑programme logs, in‑product telemetry—will feed the most granular signals into its modelling stack and obtain the sharpest audience slices. Competition in the advertising value chain therefore shifts from reach and placement inventory toward data custody and algorithmic sophistication. The firm that can extract causal preference patterns from its proprietary graph will out‑perform rivals who must rely on coarser look‑alike or contextual proxies.\n",
    "\n",
    "Consequently, future competitive advantage will be measured less by raw media spend and more by a company’s ability to fuse disparate datasets into a unified customer view and to fine‑tune or retrieve‑augment foundation models against that corpus. In practical terms, boardrooms will debate the merits of building internal LLM pipelines versus leasing external model capacity, balancing data‑sovereignty concerns against speed‑to‑market and engineering overhead. Marketing functions will need to collaborate closely with data‑engineering, legal‑compliance and machine‑learning teams—an organisational realignment that blurs the boundary between creative and technical departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ab32a-b757-427b-8ff3-ca8b01417a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
